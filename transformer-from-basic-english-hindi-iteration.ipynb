{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1355361,"sourceType":"datasetVersion","datasetId":789090,"isSourceIdPinned":false},{"sourceId":4246862,"sourceType":"datasetVersion","datasetId":2502545}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import required libraries for English-Hindi transformer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport os\n\n# Check if CUDA is available\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n\n# List files in current directory\nprint(\"\\nFiles in current directory:\")\nfor filename in os.listdir('.'):\n    print(filename)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-08-26T19:44:50.002109Z","iopub.execute_input":"2025-08-26T19:44:50.002331Z","iopub.status.idle":"2025-08-26T19:44:50.017287Z","shell.execute_reply.started":"2025-08-26T19:44:50.002315Z","shell.execute_reply":"2025-08-26T19:44:50.016612Z"},"trusted":true},"outputs":[{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA device: Tesla P100-PCIE-16GB\n\nFiles in current directory:\n.virtual_documents\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# import kagglehub\n\n# # Download latest version\n# path = kagglehub.dataset_download(\"preetviradiya/english-hindi-dataset\")\n\n# print(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T19:44:50.017856Z","iopub.execute_input":"2025-08-26T19:44:50.018018Z","iopub.status.idle":"2025-08-26T19:44:50.031831Z","shell.execute_reply.started":"2025-08-26T19:44:50.018004Z","shell.execute_reply":"2025-08-26T19:44:50.031299Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# # dataset 2\n# import kagglehub\n\n# # Download latest version\n# path = kagglehub.dataset_download(\"vaibhavkumar11/hindi-english-parallel-corpus\")\n\n# print(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T19:44:50.032415Z","iopub.execute_input":"2025-08-26T19:44:50.032593Z","iopub.status.idle":"2025-08-26T19:44:50.045888Z","shell.execute_reply.started":"2025-08-26T19:44:50.032578Z","shell.execute_reply":"2025-08-26T19:44:50.045403Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load dataset - compatible with both local and Kaggle environments\nimport os\n\n# Check if running on Kaggle or locally\nif os.path.exists('/kaggle/input'):\n    # Running on Kaggle\n    print(\"Running on Kaggle environment\")\n    # data_path = '/kaggle/input/english-hindi-dataset/Dataset_English_Hindi.csv'\n    data_path = '/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv'\nelse:\n    # Running locally\n    print(\"Running on local environment\")\n    data_path = 'Dataset_English_Hindi.csv'\n\nprint(f\"Loading data from: {data_path}\")\ndata = pd.read_csv(data_path)\nprint(f\"Dataset loaded successfully! Shape: {data.shape}\")","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:44:50.046566Z","iopub.execute_input":"2025-08-26T19:44:50.046868Z","iopub.status.idle":"2025-08-26T19:45:00.614594Z","shell.execute_reply.started":"2025-08-26T19:44:50.046850Z","shell.execute_reply":"2025-08-26T19:45:00.613932Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Running on Kaggle environment\nLoading data from: /kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv\nDataset loaded successfully! Shape: (1561841, 2)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"data.head(5)","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:45:00.616799Z","iopub.execute_input":"2025-08-26T19:45:00.617015Z","iopub.status.idle":"2025-08-26T19:45:00.637238Z","shell.execute_reply.started":"2025-08-26T19:45:00.616998Z","shell.execute_reply":"2025-08-26T19:45:00.636468Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                               hindi  \\\n0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n\n                                          english  \n0  Give your application an accessibility workout  \n1               Accerciser Accessibility Explorer  \n2  The default plugin layout for the bottom panel  \n3     The default plugin layout for the top panel  \n4  A list of plugins that are disabled by default  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hindi</th>\n      <th>english</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n      <td>Give your application an accessibility workout</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n      <td>Accerciser Accessibility Explorer</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n      <td>The default plugin layout for the bottom panel</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n      <td>The default plugin layout for the top panel</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n      <td>A list of plugins that are disabled by default</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"data.drop_duplicates(inplace = True)\ndata.dropna(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:45:00.638068Z","iopub.execute_input":"2025-08-26T19:45:00.638414Z","iopub.status.idle":"2025-08-26T19:45:01.929944Z","shell.execute_reply.started":"2025-08-26T19:45:00.638393Z","shell.execute_reply":"2025-08-26T19:45:01.929187Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"data['english'] = data['english'].str.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T19:45:01.930763Z","iopub.execute_input":"2025-08-26T19:45:01.931028Z","iopub.status.idle":"2025-08-26T19:45:02.418554Z","shell.execute_reply.started":"2025-08-26T19:45:01.931002Z","shell.execute_reply":"2025-08-26T19:45:02.417781Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"START_TOKEN = '<START>'\nPADDING_TOKEN = '<PAD>'\nEND_TOKEN = '<END>'\nUNK_TOKEN = '<UNK>'\n\nhindi_vocabulary = [\n    START_TOKEN, UNK_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', '<', '=', '>', '?', '@', '|', '।',\n    '०', '१', '२', '३', '४', '५', '६', '७', '८', '९',\n    'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ॠ', 'ऌ', 'ॡ', 'ए', 'ऐ', 'ओ', 'औ',\n    'क', 'ख', 'ग', 'घ', 'ङ',\n    'च', 'छ', 'ज', 'झ', 'ञ',\n    'ट', 'ठ', 'ड', 'ढ', 'ण',\n    'त', 'थ', 'द', 'ध', 'न',\n    'प', 'फ', 'ब', 'भ', 'म',\n    'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह',\n    'ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ', '्',\n    'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', 'य़', '…' ,'—',\n    PADDING_TOKEN, END_TOKEN\n]\n\nenglish_vocabulary = [\n    START_TOKEN, UNK_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', '<', '=', '>', '?', '@', '|',\n    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n    '[', '\\\\', ']', '^', '_', '`',\n    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n    '{', '|', '}', '~','…' ,'—',\n    PADDING_TOKEN, END_TOKEN\n]\n","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:45:02.419377Z","iopub.execute_input":"2025-08-26T19:45:02.419620Z","iopub.status.idle":"2025-08-26T19:45:02.427164Z","shell.execute_reply.started":"2025-08-26T19:45:02.419596Z","shell.execute_reply":"2025-08-26T19:45:02.426468Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T19:45:02.427961Z","iopub.execute_input":"2025-08-26T19:45:02.428227Z","iopub.status.idle":"2025-08-26T19:45:02.446818Z","shell.execute_reply.started":"2025-08-26T19:45:02.428204Z","shell.execute_reply":"2025-08-26T19:45:02.446092Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(1353877, 2)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"english_file = data['english']\nhindi_file = data['hindi']","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:45:02.447723Z","iopub.execute_input":"2025-08-26T19:45:02.448030Z","iopub.status.idle":"2025-08-26T19:45:02.462099Z","shell.execute_reply.started":"2025-08-26T19:45:02.448005Z","shell.execute_reply":"2025-08-26T19:45:02.461434Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"index_to_hindi = {k:v for k,v in enumerate(hindi_vocabulary)}\nhindi_to_index = {v:k for k,v in enumerate(hindi_vocabulary)}\n\nindex_to_english = {k:v for k,v in enumerate(english_vocabulary)}\nenglish_to_index = {v:k for k,v in enumerate(english_vocabulary)}","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:45:02.463073Z","iopub.execute_input":"2025-08-26T19:45:02.463758Z","iopub.status.idle":"2025-08-26T19:45:02.478270Z","shell.execute_reply.started":"2025-08-26T19:45:02.463730Z","shell.execute_reply":"2025-08-26T19:45:02.477521Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\nenglish_sentence = english_file\nhindi_sentence = hindi_file\nenglish_sentence = [sentence.rstrip('\\n') for sentence in english_sentence]\nhindi_sentence = [sentence.rstrip('\\n') for sentence in hindi_sentence]","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:45:02.479008Z","iopub.execute_input":"2025-08-26T19:45:02.479252Z","iopub.status.idle":"2025-08-26T19:45:02.841714Z","shell.execute_reply.started":"2025-08-26T19:45:02.479231Z","shell.execute_reply":"2025-08-26T19:45:02.840978Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"hindi_sentence[:10]","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:45:02.842419Z","iopub.execute_input":"2025-08-26T19:45:02.843001Z","iopub.status.idle":"2025-08-26T19:45:02.848074Z","shell.execute_reply.started":"2025-08-26T19:45:02.842981Z","shell.execute_reply":"2025-08-26T19:45:02.847341Z"},"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"['अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें',\n 'एक्सेर्साइसर पहुंचनीयता अन्वेषक',\n 'निचले पटल के लिए डिफोल्ट प्लग-इन खाका',\n 'ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका',\n 'उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से निष्क्रिय किया गया है',\n 'अवधि को हाइलाइट रकें',\n 'पहुंचनीय आसंधि (नोड) को चुनते समय हाइलाइट बक्से की अवधि',\n 'सीमांत (बोर्डर) के रंग को हाइलाइट करें',\n 'हाइलाइट किए गए सीमांत का रंग और अपारदर्शिता। ',\n 'भराई के रंग को हाइलाइट करें']"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"max(len(x) for x in english_sentence) , max(len(x) for x in hindi_sentence)","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:45:02.848886Z","iopub.execute_input":"2025-08-26T19:45:02.849184Z","iopub.status.idle":"2025-08-26T19:45:03.066845Z","shell.execute_reply.started":"2025-08-26T19:45:02.849123Z","shell.execute_reply":"2025-08-26T19:45:03.066131Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(11088, 8000)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"PERCENTAGE = 98\nprint(f\"{PERCENTAGE}th length of hindi word :{np.percentile([len(x) for x in hindi_sentence], PERCENTAGE)}\")\nprint(f\"{PERCENTAGE}th length of english word :{np.percentile([len(x) for x in english_sentence], PERCENTAGE)}\")","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:45:03.067858Z","iopub.execute_input":"2025-08-26T19:45:03.068117Z","iopub.status.idle":"2025-08-26T19:45:03.399341Z","shell.execute_reply.started":"2025-08-26T19:45:03.068093Z","shell.execute_reply":"2025-08-26T19:45:03.398510Z"},"trusted":true},"outputs":[{"name":"stdout","text":"98th length of hindi word :318.0\n98th length of english word :321.0\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"max_sequence_length = 300 #increase it afterward\n\n\ndef is_valid_tokens(sentence, vocab):\n    # Check if all tokens in the sentence are in the vocabulary\n    for token in list(set(sentence)):\n        if token not in vocab:\n            return False\n    return True\n\ndef is_valid_length(sentence, max_sequence_length):\n    # Check if the sentence length is within the allowed limit\n    return len(list(sentence)) < (max_sequence_length - 1)\n\nvalid_sentence_indices = []\nfor index in range(len(hindi_sentence)):\n    hindi_sent = hindi_sentence[index]\n    english_sent = english_sentence[index]\n    if (is_valid_length(hindi_sent, max_sequence_length)\n        and is_valid_length(english_sent, max_sequence_length)\n        and is_valid_tokens(hindi_sent, hindi_vocabulary)):\n        valid_sentence_indices.append(index)\n\n\nprint(f\"Number of sentences: {len(hindi_sentence)}\")\nprint(f\"Number of valid sentences: {len(valid_sentence_indices)}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:45:03.400253Z","iopub.execute_input":"2025-08-26T19:45:03.400537Z","iopub.status.idle":"2025-08-26T19:45:29.871027Z","shell.execute_reply.started":"2025-08-26T19:45:03.400515Z","shell.execute_reply":"2025-08-26T19:45:29.870429Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of sentences: 1353877\nNumber of valid sentences: 767441\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Add validation and debugging functions\ndef validate_vocabulary_coverage(sentences, vocabulary, language_name):\n    \"\"\"Check if all characters in sentences are covered by vocabulary\"\"\"\n    all_chars = set()\n    for sentence in sentences:\n        all_chars.update(set(sentence))\n    \n    missing_chars = all_chars - set(vocabulary)\n    if missing_chars:\n        print(f\"Missing characters in {language_name} vocabulary: {missing_chars}\")\n        return False\n    return True\n\ndef clean_sentences(sentences, vocabulary, unk_token):\n    \"\"\"Replace unknown characters with UNK token\"\"\"\n    cleaned = []\n    vocab_set = set(vocabulary)\n    for sentence in sentences:\n        cleaned_sentence = \"\"\n        for char in sentence:\n            if char in vocab_set:\n                cleaned_sentence += char\n            else:\n                cleaned_sentence += unk_token\n        cleaned.append(cleaned_sentence)\n    return cleaned\n\n# Validate vocabulary coverage\nprint(\"Validating vocabulary coverage...\")\nhindi_valid = validate_vocabulary_coverage(hindi_sentence, hindi_vocabulary, \"Hindi\")\nenglish_valid = validate_vocabulary_coverage(english_sentence, english_vocabulary, \"English\")\n\nif not hindi_valid:\n    print(\"Cleaning Hindi sentences...\")\n    hindi_sentence = clean_sentences(hindi_sentence, hindi_vocabulary, UNK_TOKEN)\n\nif not english_valid:\n    print(\"Cleaning English sentences...\")\n    english_sentence = clean_sentences(english_sentence, english_vocabulary, UNK_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T19:45:29.871718Z","iopub.execute_input":"2025-08-26T19:45:29.871913Z","iopub.status.idle":"2025-08-26T19:46:00.222046Z","shell.execute_reply.started":"2025-08-26T19:45:29.871890Z","shell.execute_reply":"2025-08-26T19:46:00.221363Z"}},"outputs":[{"name":"stdout","text":"Validating vocabulary coverage...\nMissing characters in Hindi vocabulary: {'Ô', '¡', '²', '€', 'а', '\\u2003', ';', 'é', '•', '¥', '×', 'آ', '中', 'ت', 'Π', 'þ', '⁶', '⁰', 'θ', 'u', 'w', '⌊', 'র', '॒', '2', 'L', 'ö', '”', '∞', '设', '￼', '॰', '♫', '\\x89', 'Ż', '॔', 'Ç', '–', '―', 'č', 'I', 'ુ', '‘', 'β', '́', '̀', 'µ', 'Ň', '˙', '’', 'ţ', '\\x98', '0', '़', '⁵', 'У', 'ॅ', '≤', 'দ', '،', 'd', '\\u200d', '∂', 'ॊ', '漢', 'ü', 'G', 'и', 'か', 'Ś', '\\x81', 'b', '\\uf00f', '¶', '°', 'ā', '字', '\\x80', 'ॐ', 'Ă', '\\x8a', '\\x86', 'M', 'Æ', 'j', '˚', 'р', '⅗', '´', 'ॄ', '⅓', '֦', 'ś', 'Ě', '̈', '⅕', '₂', 'λ', 'ç', '→', '˝', '√', 'ی', 'ď', 'ů', 'ù', 'ृ', '\\x88', '̄', 'Ð', '\\x9b', '\\x9f', 'ج', '\\x87', 'ॉ', 'া', 'ř', '³', 'Ĺ', 'ق', 'Θ', '6', '5', '̪', 'Ą', 's', '∕', '¤', 'ж', '4', 'º', 'ٓ', '₁', 'α', '\\x94', 'Ω', '\\x9a', '\\x93', 'h', 'ر', 't', '8', 'c', 'N', '☺', 'ী', 'ক', 'Ť', '{', '\\x83', 'Ó', 'ι', 'μ', 'R', '⌈', 'q', 'ω', '£', 'x', 'i', 'κ', 'е', '“', 'م', '⏎', 'k', 'ু', 'о', 'm', 'ਿ', '¹', '\\uf0a7', 'ʼ', 'Ѕ', 'ť', 'Ę', 'ł', 'J', '∣', 'W', 'γ', 'ਲ', '¼', '̧', 'س', '¾', '\\x90', 'ŕ', 'Ž', '⇒', '7', 'ı', 'Ê', 'e', '₀', '¦', 'ě', '‡', 'ل', '⁷', 'ó', 'ੀ', 'ż', 'ψ', 'А', 'н', '[', 'ş', '\\x8f', 'ভ', 'Â', 'থ', 'Ć', 'ऑ', '−', 'Ţ', '\\x8c', '÷', '¯', '堀', 'Ľ', '½', '॓', '\\uf008', 'Ş', 'ͧ', 'Ü', '†', '\\x9c', 'ٕ', 'Š', 'U', 'H', '⁴', 'ú', '\\x14', 'l', '♪', '\\x91', '≥', 'V', 'C', 'স', '~', '⌘', 'ő', 'ঠ', 'p', '\\x1b', '`', ';', 'ऽ', 'Č', 'ɓ', 'પ', '\\x85', 'n', '®', '\\x96', 'ž', '̃', '™', 'ɪ', 'r', 'v', '˛', 'ю', 'ੱ', '⅖', 'E', 'S', 'B', 'д', '₹', 'ন', '¿', 'í', '⌉', 'õ', 'y', '\\u200e', '\\t', '1', '�', '¢', 'な', 'Т', 'O', 'С', 'a', '±', 'π', 'å', 'Y', '«', '\\x82', 'ॆ', 'g', 'à', '國', 'п', '̊', 'K', 'ñ', '⌋', 'Z', '9', '„', 'τ', 'ਦ', '¨', 'Î', 'ň', '\\\\', '©', 'ľ', '\\x9d', 'Û', 'ĺ', '্', '\\x95', '¬', 'চ', '⅙', 'X', 'ऍ', 'ح', 'o', 'z', '∨', 'ώ', '➣', 'د', 'Ч', '\\x8b', 'Ú', '′', 'ॣ', '■', 'Δ', 'â', '≠', '॑', '\\x99', 'P', 'ν', 'ý', 'š', 'є', 'T', 'æ', '⁻', '׃', '\\x8d', 'è', 'в', '_', 'ò', 'ː', 'બ', 'D', 'A', 'ä', 'ֹ', 'É', '3', '§', 'á', 'ˇ', 'ű', 'ऒ', 'ا', '秦', 'ª', '─', 'Λ', '⅘', 'Д', 'ষ', 'ن', 'й', 'Ö', '\\x9e', 'إ', '^', 'ć', 'Q', 'ळ', '☻', '═', ']', 'f', '⅔', 'ą', '続', 'F', 'ऎ', '·', 'φ', '\\x97', '¸', '}', '॥', 'ા', 'η', 'Ä', 'ব'}\nMissing characters in English vocabulary: {'ț', 'ờ', 'ṃ', '¡', '²', 'य', '€', 'а', 'ू', ';', 'é', '•', '×', 'ʿ', 'ت', 'ي', '̇', '⁶', '⁰', 'θ', 'ģ', '⌊', 'ρ', 'ī', 'র', 'ö', 'ज', 'ḥ', '‰', 'ة', '”', '∞', '揺', 'ण', 'ο', '॰', '♫', '\\x89', '\\x84', 'ई', '–', '―', 'č', 'ǁ', '‘', 'β', '́', 'ê', 'µ', 'ô', 'ţ', '’', 'ड', '\\x98', 'ļ', '⁵', '़', 'ॅ', '≤', '८', '˘', 'দ', '漢', 'ü', '»', 'ủ', 'か', '\\ufeff', 'क', 'ओ', '७', '\\x81', '°', 'ā', '謡', '字', '\\x80', 'ë', 'ū', '←', '\\u200c', '˚', 'р', '⅗', '´', 'ऐ', '⅓', 'ो', '葉', '曜', 'ś', '̈', '⅕', '₂', 'λ', 'ç', '→', '√', 'ल', 'झ', 'ृ', 'ौ', 'ğ', 'व', 'ï', 'ॉ', 'ř', 'া', '³', '५', 'त', 'ō', '遥', 'च', '∕', '¤', 'º', 'ǆ', '₁', 'α', '\\xa0', 'द', '\\x94', '३', 'न', 'đ', 'ر', '☺', 'ञ', 'ऊ', 'ų', 'ι', '\\u200b', 'μ', 'ω', '⌈', '£', 'उ', 'κ', 'ठ', 'छ', 'е', '“', 'ș', 'م', 'ः', 'ग', 'ু', 'о', '्', 'प', 'म', 'स', '¹', '\\uf0a7', 'ध', 'ł', '∣', 'γ', 'ा', 'ð', 'ṣ', '\\u2028', '¼', 'س', 'ऋ', '¾', 'श', '‐', '様', 'ु', 'ό', 'ø', 'ı', 'î', '容', 'ě', '०', '₀', 'ह', 'ل', 'ó', 'ﬁ', '⁷', 'ṭ', 'ż', 'ү', 'н', 'ş', 'ं', 'ভ', 'ऑ', '−', '羊', '\\x8c', '÷', '⋯', 'ḷ', '½', 'δ', '\\x92', 'ﬂ', 'ę', 'थ', '†', '\\x9c', '⁴', 'ú', '♪', '≥', 'أ', 'ζ', 'স', '妖', '⌘', '由', 'ő', '째', '\\x1b', 'आ', 'ṇ', 'ư', '®', 'ž', '™', 'σ', '˛', 'ί', 'ã', '⅖', 'д', '₹', 'ন', '有', 'ि', 'í', '⌉', 'õ', '\\u200e', 'औ', '溶', '�', 'な', '\\t', 'घ', '不', '±', 'π', 'å', '।', '«', '\\x82', '熔', 'अ', '६', '१', 'à', 'इ', 'п', '\\u200f', 'ă', '耀', 'ñ', '⌋', '„', 'τ', 'ẻ', '¨', '傭', 'ň', '©', 'ē', 'ľ', '্', '\\x95', 'ì', 'চ', '¬', 'े', 'ə', '⅙', 'ब', 'ऍ', 'ŭ', '४', 'ح', '付', 'ч', '\\xad', '∨', 'ε', '➣', 'د', '用', '庸', '′', '擁', 'ी', '२', 'फ', 'ए', 'भ', 'ß', 'ष', '∶', 'â', '≠', '\\x99', 'ν', 'ý', 'š', 'ė', 'æ', '⁻', '預', '\\x8d', 'è', 'ä', 'ò', 'в', 'ँ', '悠', 'ĭ', '§', 'á', 'ź', 'ट', '९', 'ख', 'ऒ', 'ا', '病', 'र', 'ṛ', 'ŏ', 'ª', '─', '⅘', 'ষ', '要', 'ń', 'й', 'إ', '洋', '֠', 'ć', 'ṅ', 'ḍ', 'و', '☻', '⅔', '続', 'ơ', 'φ', '·', 'ै', 'η', '¸', '憂', 'ব', 'ك'}\nCleaning Hindi sentences...\nCleaning English sentences...\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"hindi_sentence = [hindi_sentence[i] for i in valid_sentence_indices]\nenglish_sentence = [english_sentence[i] for i in valid_sentence_indices]","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:00.222778Z","iopub.execute_input":"2025-08-26T19:46:00.223076Z","iopub.status.idle":"2025-08-26T19:46:00.363861Z","shell.execute_reply.started":"2025-08-26T19:46:00.223053Z","shell.execute_reply":"2025-08-26T19:46:00.363340Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"print(len(hindi_sentence))\nprint(len(english_sentence))","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:00.364868Z","iopub.execute_input":"2025-08-26T19:46:00.365116Z","iopub.status.idle":"2025-08-26T19:46:00.368991Z","shell.execute_reply.started":"2025-08-26T19:46:00.365094Z","shell.execute_reply":"2025-08-26T19:46:00.368290Z"},"trusted":true},"outputs":[{"name":"stdout","text":"767441\n767441\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass TextDataset(Dataset):\n    def __init__(self, english_sentence, hindi_sentence):\n        self.english_sentence = english_sentence\n        self.hindi_sentence = hindi_sentence\n\n    def __len__(self):\n        return len(self.english_sentence)\n\n    def __getitem__(self, index):\n        english_sentence = self.english_sentence[index]\n        hindi_sentence = self.hindi_sentence[index]\n        return english_sentence, hindi_sentence\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:00.371949Z","iopub.execute_input":"2025-08-26T19:46:00.372380Z","iopub.status.idle":"2025-08-26T19:46:00.387063Z","shell.execute_reply.started":"2025-08-26T19:46:00.372352Z","shell.execute_reply":"2025-08-26T19:46:00.386454Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"dataset = TextDataset(english_sentence,hindi_sentence)","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:00.387767Z","iopub.execute_input":"2025-08-26T19:46:00.387936Z","iopub.status.idle":"2025-08-26T19:46:00.404979Z","shell.execute_reply.started":"2025-08-26T19:46:00.387922Z","shell.execute_reply":"2025-08-26T19:46:00.404468Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"batch_size = 3\ntrain_loader = DataLoader(dataset, batch_size)\niterator = iter(train_loader)","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:00.405640Z","iopub.execute_input":"2025-08-26T19:46:00.405871Z","iopub.status.idle":"2025-08-26T19:46:00.440848Z","shell.execute_reply.started":"2025-08-26T19:46:00.405846Z","shell.execute_reply":"2025-08-26T19:46:00.440184Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"for batch_num, batch in enumerate(iterator):\n    print(batch)\n    if batch_num > 3:\n        break","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:00.441612Z","iopub.execute_input":"2025-08-26T19:46:00.441889Z","iopub.status.idle":"2025-08-26T19:46:00.448052Z","shell.execute_reply.started":"2025-08-26T19:46:00.441865Z","shell.execute_reply":"2025-08-26T19:46:00.447393Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[('give your application an accessibility workout', 'accerciser accessibility explorer', 'the default plugin layout for the bottom panel'), ('अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें', 'एक्सेर्साइसर पहुंचनीयता अन्वेषक', 'निचले पटल के लिए डिफोल्ट प्लग-इन खाका')]\n[('the default plugin layout for the top panel', 'a list of plugins that are disabled by default', 'highlight duration'), ('ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका', 'उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से निष्क्रिय किया गया है', 'अवधि को हाइलाइट रकें')]\n[('the duration of the highlight box when selecting accessible nodes', 'highlight border color', 'the color and opacity of the highlight border.'), ('पहुंचनीय आसंधि (नोड) को चुनते समय हाइलाइट बक्से की अवधि', 'सीमांत (बोर्डर) के रंग को हाइलाइट करें', 'हाइलाइट किए गए सीमांत का रंग और अपारदर्शिता। ')]\n[('highlight fill color', 'the color and opacity of the highlight fill.', 'api browser'), ('भराई के रंग को हाइलाइट करें', 'हाइलाइट किया गया भराई का रंग और पारदर्शिता। ', 'एपीआई विचरक')]\n[('browse the various methods of the current accessible', 'hide private attributes', 'method'), ('इस समय जिसे प्राप्त किया गया हो, उसकी विभिन्न विधियों (मेथड) में विचरण करें', 'निजी गुणों को छिपाएं', 'विधि')]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"def tokenize(sentence, language_to_index, start_token=True, end_token=True):\n    sentence_word_indicies = [language_to_index[token] for token in list(sentence)]\n    if start_token:\n        sentence_word_indicies.insert(0, language_to_index[START_TOKEN])\n    if end_token:\n        sentence_word_indicies.append(language_to_index[END_TOKEN])\n    for _ in range(len(sentence_word_indicies), max_sequence_length):\n        sentence_word_indicies.append(language_to_index[PADDING_TOKEN])\n    return torch.tensor(sentence_word_indicies)","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:00.448956Z","iopub.execute_input":"2025-08-26T19:46:00.449209Z","iopub.status.idle":"2025-08-26T19:46:00.463750Z","shell.execute_reply.started":"2025-08-26T19:46:00.449184Z","shell.execute_reply":"2025-08-26T19:46:00.463058Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"batch","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:00.464421Z","iopub.execute_input":"2025-08-26T19:46:00.464637Z","iopub.status.idle":"2025-08-26T19:46:00.477755Z","shell.execute_reply.started":"2025-08-26T19:46:00.464621Z","shell.execute_reply":"2025-08-26T19:46:00.477214Z"},"trusted":true},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[('browse the various methods of the current accessible',\n  'hide private attributes',\n  'method'),\n ('इस समय जिसे प्राप्त किया गया हो, उसकी विभिन्न विधियों (मेथड) में विचरण करें',\n  'निजी गुणों को छिपाएं',\n  'विधि')]"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eng_tokenized, hin_tokenized = [], []\nfor sentence_num in range(batch_size):\n    eng_sentence, hin_sentence = batch[0][sentence_num], batch[1][sentence_num]\n    eng_tokenized.append( tokenize(eng_sentence, english_to_index, start_token=False, end_token=False) )\n    hin_tokenized.append( tokenize(hin_sentence, hindi_to_index, start_token=True, end_token=True) )\neng_tokenized = torch.stack(eng_tokenized)\nhin_tokenized = torch.stack(hin_tokenized)","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:00.478356Z","iopub.execute_input":"2025-08-26T19:46:00.478556Z","iopub.status.idle":"2025-08-26T19:46:00.563165Z","shell.execute_reply.started":"2025-08-26T19:46:00.478541Z","shell.execute_reply":"2025-08-26T19:46:00.562414Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### Transformer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport math\nfrom torch import nn\nimport torch.nn.functional as F\n\ndef get_device():\n    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndef scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1]\n    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n    if mask is not None:\n        scaled = scaled.permute(1, 0, 2, 3) + mask\n        scaled = scaled.permute(1, 0, 2, 3)\n    attention = F.softmax(scaled, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_sequence_length):\n        super().__init__()\n        self.max_sequence_length = max_sequence_length\n        self.d_model = d_model\n\n    def forward(self, device):\n        even_i = torch.arange(0, self.d_model, 2).float().to(device)\n        denominator = torch.pow(10000, even_i/self.d_model)\n        position = (torch.arange(self.max_sequence_length, device=device)\n                          .reshape(self.max_sequence_length, 1))\n        even_PE = torch.sin(position / denominator)\n        odd_PE = torch.cos(position / denominator)\n        stacked = torch.stack([even_PE, odd_PE], dim=2)\n        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n        return PE\n\nclass SentenceEmbedding(nn.Module):\n    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n        super().__init__()\n        self.vocab_size = len(language_to_index)\n        self.max_sequence_length = max_sequence_length\n        self.embedding = nn.Embedding(self.vocab_size, d_model)\n        self.language_to_index = language_to_index\n        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n        self.dropout = nn.Dropout(p=0.1)\n        self.START_TOKEN = START_TOKEN\n        self.END_TOKEN = END_TOKEN\n        self.PADDING_TOKEN = PADDING_TOKEN\n\n    def batch_tokenize(self, batch, start_token, end_token):\n        # Get the device from the embedding layer\n        device = next(self.parameters()).device\n\n        def tokenize(sentence, start_token, end_token):\n            sentence_word_indicies = []\n            for token in list(sentence):\n                if token in self.language_to_index:\n                    sentence_word_indicies.append(self.language_to_index[token])\n                else:\n                    # Handle unknown tokens\n                    sentence_word_indicies.append(self.language_to_index.get('<UNK>', 1))\n            \n            if start_token:\n                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n            if end_token:\n                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n            \n            # Truncate if too long\n            if len(sentence_word_indicies) > self.max_sequence_length:\n                sentence_word_indicies = sentence_word_indicies[:self.max_sequence_length-1]\n                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n            \n            # Pad to max length\n            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n            \n            # Validate indices\n            for idx in sentence_word_indicies:\n                if idx >= self.vocab_size or idx < 0:\n                    raise ValueError(f\"Invalid token index: {idx}\")\n            \n            return torch.tensor(sentence_word_indicies, device=device)\n\n        tokenized = []\n        for sentence_num in range(len(batch)):\n           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n        tokenized = torch.stack(tokenized)\n        return tokenized\n\n    def forward(self, x, start_token, end_token): # sentence\n        device = next(self.parameters()).device\n        x = self.batch_tokenize(x, start_token, end_token)\n        x = self.embedding(x)\n        pos = self.position_encoder(device)\n        x = self.dropout(x + pos)\n        return x\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n\n    def forward(self, x, mask):\n        batch_size, sequence_length, d_model = x.size()\n        qkv = self.qkv_layer(x)\n        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3)\n        q, k, v = qkv.chunk(3, dim=-1)\n        values, attention = scaled_dot_product(q, k, v, mask)\n        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n        out = self.linear_layer(values)\n        return out\n\n\nclass LayerNormalization(nn.Module):\n    def __init__(self, parameters_shape, eps=1e-5):\n        super().__init__()\n        self.parameters_shape=parameters_shape\n        self.eps=eps\n        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n\n    def forward(self, inputs):\n        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n        mean = inputs.mean(dim=dims, keepdim=True)\n        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n        std = (var + self.eps).sqrt()\n        y = (inputs - mean) / std\n        out = self.gamma * y + self.beta\n        return out\n\n\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, hidden, drop_prob=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, hidden)\n        self.linear2 = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(EncoderLayer, self).__init__()\n        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, self_attention_mask):\n        residual_x = x.clone()\n        x = self.attention(x, mask=self_attention_mask)\n        x = self.dropout1(x)\n        x = self.norm1(x + residual_x)\n        residual_x = x.clone()\n        x = self.ffn(x)\n        x = self.dropout2(x)\n        x = self.norm2(x + residual_x)\n        return x\n\nclass SequentialEncoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, self_attention_mask  = inputs\n        for module in self._modules.values():\n            x = module(x, self_attention_mask)\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self,\n                 d_model,\n                 ffn_hidden,\n                 num_heads,\n                 drop_prob,\n                 num_layers,\n                 max_sequence_length,\n                 language_to_index,\n                 START_TOKEN,\n                 END_TOKEN,\n                 PADDING_TOKEN):\n        super().__init__()\n        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n                                      for _ in range(num_layers)])\n\n    def forward(self, x, self_attention_mask, start_token, end_token):\n        x = self.sentence_embedding(x, start_token, end_token)\n        x = self.layers(x, self_attention_mask)\n        return x\n\n\nclass MultiHeadCrossAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n        self.q_layer = nn.Linear(d_model , d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n\n    def forward(self, x, y, mask):\n        batch_size, sequence_length, d_model = x.size()\n        kv = self.kv_layer(x)\n        q = self.q_layer(y)\n        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n        kv = kv.permute(0, 2, 1, 3)\n        q = q.permute(0, 2, 1, 3)\n        k, v = kv.chunk(2, dim=-1)\n        values, attention = scaled_dot_product(q, k, v, mask)\n        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n        out = self.linear_layer(values)\n        return out\n\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(DecoderLayer, self).__init__()\n        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n\n        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout3 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n        _y = y.clone()\n        y = self.self_attention(y, mask=self_attention_mask)\n        y = self.dropout1(y)\n        y = self.layer_norm1(y + _y)\n\n        _y = y.clone()\n        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n        y = self.dropout2(y)\n        y = self.layer_norm2(y + _y)\n\n        _y = y.clone()\n        y = self.ffn(y)\n        y = self.dropout3(y)\n        y = self.layer_norm3(y + _y)\n        return y\n\n\nclass SequentialDecoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, y, self_attention_mask, cross_attention_mask = inputs\n        for module in self._modules.values():\n            y = module(x, y, self_attention_mask, cross_attention_mask)\n        return y\n\nclass Decoder(nn.Module):\n    def __init__(self,\n                 d_model,\n                 ffn_hidden,\n                 num_heads,\n                 drop_prob,\n                 num_layers,\n                 max_sequence_length,\n                 language_to_index,\n                 START_TOKEN,\n                 END_TOKEN,\n                 PADDING_TOKEN):\n        super().__init__()\n        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n\n    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n        y = self.sentence_embedding(y, start_token, end_token)\n        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n        return y\n\n\nclass Transformer(nn.Module):\n    def __init__(self,\n                d_model,\n                ffn_hidden,\n                num_heads,\n                drop_prob,\n                num_layers,\n                max_sequence_length,\n                hin_vocab_size,\n                english_to_index,\n                hindi_to_index,\n                START_TOKEN,\n                END_TOKEN,\n                PADDING_TOKEN\n                ):\n        super().__init__()\n        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, hindi_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.linear = nn.Linear(d_model, hin_vocab_size)\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    def forward(self,\n                x,\n                y,\n                encoder_self_attention_mask=None,\n                decoder_self_attention_mask=None,\n                decoder_cross_attention_mask=None,\n                enc_start_token=False,\n                enc_end_token=False,\n                dec_start_token=False,\n                dec_end_token=False):\n        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n        out = self.linear(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:00.564224Z","iopub.execute_input":"2025-08-26T19:46:00.564500Z","iopub.status.idle":"2025-08-26T19:46:00.815914Z","shell.execute_reply.started":"2025-08-26T19:46:00.564475Z","shell.execute_reply":"2025-08-26T19:46:00.815190Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### Translation ","metadata":{}},{"cell_type":"code","source":"\nd_model = 512\nbatch_size = 30\nffn_hidden = 2048\nnum_heads = 8\ndrop_prob = 0.1\nnum_layers = 3 #increase it afterward\nmax_sequence_length = 300 #increase it afterward\nhin_vocab_size = len(hindi_vocabulary)\n\ntransformer = Transformer(d_model,\n                          ffn_hidden,\n                          num_heads,\n                          drop_prob,\n                          num_layers,\n                          max_sequence_length,\n                          hin_vocab_size,\n                          english_to_index,\n                          hindi_to_index,\n                          START_TOKEN,\n                          END_TOKEN,\n                          PADDING_TOKEN)","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:00.816789Z","iopub.execute_input":"2025-08-26T19:46:00.817049Z","iopub.status.idle":"2025-08-26T19:46:01.009675Z","shell.execute_reply.started":"2025-08-26T19:46:00.817024Z","shell.execute_reply":"2025-08-26T19:46:01.009102Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"transformer","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:01.010354Z","iopub.execute_input":"2025-08-26T19:46:01.010598Z","iopub.status.idle":"2025-08-26T19:46:01.016360Z","shell.execute_reply.started":"2025-08-26T19:46:01.010574Z","shell.execute_reply":"2025-08-26T19:46:01.015588Z"},"trusted":true},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Transformer(\n  (encoder): Encoder(\n    (sentence_embedding): SentenceEmbedding(\n      (embedding): Embedding(100, 512)\n      (position_encoder): PositionalEncoding()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (layers): SequentialEncoder(\n      (0): EncoderLayer(\n        (attention): MultiHeadAttention(\n          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (norm1): LayerNormalization()\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (norm2): LayerNormalization()\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): EncoderLayer(\n        (attention): MultiHeadAttention(\n          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (norm1): LayerNormalization()\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (norm2): LayerNormalization()\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): EncoderLayer(\n        (attention): MultiHeadAttention(\n          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (norm1): LayerNormalization()\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (norm2): LayerNormalization()\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): Decoder(\n    (sentence_embedding): SentenceEmbedding(\n      (embedding): Embedding(108, 512)\n      (position_encoder): PositionalEncoding()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (layers): SequentialDecoder(\n      (0): DecoderLayer(\n        (self_attention): MultiHeadAttention(\n          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (layer_norm1): LayerNormalization()\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (encoder_decoder_attention): MultiHeadCrossAttention(\n          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (layer_norm2): LayerNormalization()\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (layer_norm3): LayerNormalization()\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (1): DecoderLayer(\n        (self_attention): MultiHeadAttention(\n          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (layer_norm1): LayerNormalization()\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (encoder_decoder_attention): MultiHeadCrossAttention(\n          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (layer_norm2): LayerNormalization()\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (layer_norm3): LayerNormalization()\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (2): DecoderLayer(\n        (self_attention): MultiHeadAttention(\n          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (layer_norm1): LayerNormalization()\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (encoder_decoder_attention): MultiHeadCrossAttention(\n          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (layer_norm2): LayerNormalization()\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (layer_norm3): LayerNormalization()\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (linear): Linear(in_features=512, out_features=108, bias=True)\n)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"dataset = TextDataset(english_sentence, hindi_sentence)\n","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:01.017091Z","iopub.execute_input":"2025-08-26T19:46:01.017405Z","iopub.status.idle":"2025-08-26T19:46:01.027591Z","shell.execute_reply.started":"2025-08-26T19:46:01.017383Z","shell.execute_reply":"2025-08-26T19:46:01.027058Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"len(dataset)\n","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:01.028456Z","iopub.execute_input":"2025-08-26T19:46:01.028773Z","iopub.status.idle":"2025-08-26T19:46:01.042100Z","shell.execute_reply.started":"2025-08-26T19:46:01.028753Z","shell.execute_reply":"2025-08-26T19:46:01.041397Z"},"trusted":true},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"767441"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"train_loader = DataLoader(dataset, batch_size)\niterator = iter(train_loader)","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:01.042733Z","iopub.execute_input":"2025-08-26T19:46:01.042920Z","iopub.status.idle":"2025-08-26T19:46:01.055440Z","shell.execute_reply.started":"2025-08-26T19:46:01.042899Z","shell.execute_reply":"2025-08-26T19:46:01.054892Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from torch import nn\n\ncriterian = nn.CrossEntropyLoss(ignore_index = hindi_to_index[PADDING_TOKEN],\n                                reduction='none')\n\nfor params in transformer.parameters():\n    if params.dim() > 1:\n        nn.init.xavier_uniform_(params)\n\noptim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:01.056016Z","iopub.execute_input":"2025-08-26T19:46:01.056255Z","iopub.status.idle":"2025-08-26T19:46:03.717730Z","shell.execute_reply.started":"2025-08-26T19:46:01.056235Z","shell.execute_reply":"2025-08-26T19:46:03.717194Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"NEG_INFTY = -1e9\n\ndef create_masks(eng_batch, hin_batch):\n    num_sentences = len(eng_batch)\n    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length], True)\n    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n\n    for idx in range(num_sentences):\n        eng_sentence_length, hin_sentence_length = len(eng_batch[idx]), len(hin_batch[idx])\n        eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n        hin_chars_to_padding_mask = np.arange(hin_sentence_length + 1, max_sequence_length)\n        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n        decoder_padding_mask_self_attention[idx, :, hin_chars_to_padding_mask] = True\n        decoder_padding_mask_self_attention[idx, hin_chars_to_padding_mask, :] = True\n        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n        decoder_padding_mask_cross_attention[idx, hin_chars_to_padding_mask, :] = True\n\n    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n    decoder_self_attention_mask = torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask","metadata":{"execution":{"iopub.status.busy":"2025-08-26T19:46:03.718413Z","iopub.execute_input":"2025-08-26T19:46:03.718782Z","iopub.status.idle":"2025-08-26T19:46:03.725010Z","shell.execute_reply.started":"2025-08-26T19:46:03.718751Z","shell.execute_reply":"2025-08-26T19:46:03.724255Z"},"trusted":true},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# CUDA-optimized training with proper device management\nprint(\"Available device:\", get_device())\n\n# Use CUDA if available, otherwise CPU\ndevice = get_device()\nprint(f\"Using device: {device}\")\n\n# Use a smaller subset for initial testing\nprint(\"Using a smaller subset for faster training...\")\ndataset_size =  min(10000, len(dataset))  #use like size:batch = 10000,32 or 50000,64 etc if all(list(range(total_size)))\nsubset_indices = torch.randperm(len(dataset))[:dataset_size]\nsubset_english = [english_sentence[i] for i in subset_indices]\nsubset_hindi = [hindi_sentence[i] for i in subset_indices]\ntest_dataset = TextDataset(subset_english, subset_hindi)\n\n# Use smaller batch size for testing\ntest_batch_size = 64\ntest_loader = DataLoader(test_dataset, test_batch_size)\n\nprint(\"Testing model setup...\")\ntransformer.train()\n\n# Move model to device FIRST\ntransformer.to(device)\nprint(f\"Model moved to {device}\")\n\n# Try a small test first\ntest_iterator = iter(test_loader)\ntest_batch = next(test_iterator)\n\ntry:\n    eng_batch, hin_batch = test_batch\n    print(f\"Test English sentence: {eng_batch[0][:50]}...\")\n    print(f\"Test Hindi sentence: {hin_batch[0][:50]}...\")\n    \n    # Test tokenization\n    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, hin_batch)\n    \n    with torch.no_grad():\n        test_output = transformer(eng_batch,\n                                 hin_batch,\n                                 encoder_self_attention_mask.to(device),\n                                 decoder_self_attention_mask.to(device),\n                                 decoder_cross_attention_mask.to(device),\n                                 enc_start_token=False,\n                                 enc_end_token=False,\n                                 dec_start_token=True,\n                                 dec_end_token=True)\n    \n    print(\"Device test successful! Model output shape:\", test_output.shape)\n    print(f\"Training on {device}\")\n    \nexcept Exception as e:\n    print(f\"Error during testing: {e}\")\n    print(\"Falling back to CPU...\")\n    device = torch.device('cpu')\n    transformer.to(device)\n\n# Training loop with proper device management\ntotal_loss = 0\nnum_epochs = 20 # Start with just 2 epochs for testing\nhin_vocab_size = len(hindi_vocabulary)\n\nprint(f\"Starting training on {device}\")\nprint(f\"Vocab sizes - English: {len(english_vocabulary)}, Hindi: {len(hindi_vocabulary)}\")\nprint(f\"Dataset size: {len(test_dataset)} samples\")\n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    iterator = iter(test_loader)\n    epoch_loss = 0\n    batch_count = 0\n    \n    for batch_num, batch in enumerate(iterator):\n        try:\n            transformer.train()\n            eng_batch, hin_batch = batch\n            \n            # Validate batch\n            if len(eng_batch) == 0 or len(hin_batch) == 0:\n                continue\n                \n            # Create masks and move to device\n            encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, hin_batch)\n            encoder_self_attention_mask = encoder_self_attention_mask.to(device)\n            decoder_self_attention_mask = decoder_self_attention_mask.to(device)\n            decoder_cross_attention_mask = decoder_cross_attention_mask.to(device)\n            \n            optim.zero_grad()\n            \n            hin_predictions = transformer(eng_batch,\n                                         hin_batch,\n                                         encoder_self_attention_mask,\n                                         decoder_self_attention_mask,\n                                         decoder_cross_attention_mask,\n                                         enc_start_token=False,\n                                         enc_end_token=False,\n                                         dec_start_token=True,\n                                         dec_end_token=True)\n            \n            labels = transformer.decoder.sentence_embedding.batch_tokenize(hin_batch, start_token=False, end_token=True)\n            \n            loss = criterian(\n                hin_predictions.view(-1, hin_vocab_size),\n                labels.view(-1)\n            )\n            \n            valid_indicies = torch.where(labels.view(-1) == hindi_to_index[PADDING_TOKEN], False, True)\n            loss = loss.sum() / valid_indicies.sum()\n            \n            loss.backward()\n            optim.step()\n            \n            epoch_loss += loss.item()\n            batch_count += 1\n            \n            if batch_num % 100 == 0:  # Show progress every 10,100,1000batches\n                print(f\"  Batch {batch_num}: Loss = {loss.item():.4f}\")\n                print(f\"  English: {eng_batch[0][:30]}...\")\n                print(f\"  Hindi: {hin_batch[0][:30]}...\")\n                \n                # Show prediction for first sentence\n                hin_sentence_predicted = torch.argmax(hin_predictions[0], axis=1)\n                predicted_sentence = \"\"\n                for idx in hin_sentence_predicted:\n                  if idx == hindi_to_index[END_TOKEN]:\n                    break\n                  predicted_sentence += index_to_hindi[idx.item()]\n                print(f\"  Prediction: {predicted_sentence[:30]}...\")\n                print(\"  \" + \"-\" * 40)\n        \n        except Exception as e:\n            print(f\"Error in batch {batch_num}: {e}\")\n            continue\n    \n    avg_loss = epoch_loss / max(batch_count, 1)\n    print(f\"Epoch {epoch + 1} completed. Average loss: {avg_loss:.4f}\")\n    print(\"=\" * 50)\n\nprint(\"\\nTraining completed!\")\nprint(\"You can now test the model or continue training with more epochs and larger datasets.\")\n\n# Simple translation test\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING THE MODEL\")\nprint(\"=\"*60)\n\ntransformer.eval()\nwith torch.no_grad():\n    test_sentence = \"hello how are you\"\n    print(f\"Input: {test_sentence}\")\n    \n    # Generate translation\n    hin_sentence = (\"\",)\n    eng_sentence = (test_sentence,)\n    \n    for word_counter in range(min(50, max_sequence_length)):\n        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_sentence, hin_sentence)\n        predictions = transformer(eng_sentence,\n                                  hin_sentence,\n                                  encoder_self_attention_mask.to(device),\n                                  decoder_self_attention_mask.to(device),\n                                  decoder_cross_attention_mask.to(device),\n                                  enc_start_token=False,\n                                  enc_end_token=False,\n                                  dec_start_token=True,\n                                  dec_end_token=False)\n        next_token_prob_distribution = predictions[0][word_counter]\n        next_token_index = torch.argmax(next_token_prob_distribution).item()\n        next_token = index_to_hindi[next_token_index]\n        hin_sentence = (hin_sentence[0] + next_token, )\n        if next_token == END_TOKEN:\n            break\n    \n    print(f\"Translation: {hin_sentence[0]}\")\n    print(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T19:46:22.133875Z","iopub.execute_input":"2025-08-26T19:46:22.134194Z","iopub.status.idle":"2025-08-26T20:23:05.176746Z","shell.execute_reply.started":"2025-08-26T19:46:22.134140Z","shell.execute_reply":"2025-08-26T20:23:05.175925Z"}},"outputs":[{"name":"stdout","text":"Available device: cuda\nUsing device: cuda\nUsing a smaller subset for faster training...\nTesting model setup...\nModel moved to cuda\nTest English sentence: indeed the companions of paradise, that day, will ...\nTest Hindi sentence: निश्चय ही जन्नतवाले आज किसी न किसी काम नें व्यस्त ...\nDevice test successful! Model output shape: torch.Size([64, 300, 108])\nTraining on cuda\nStarting training on cuda\nVocab sizes - English: 101, Hindi: 108\nDataset size: 10000 samples\n\nEpoch 1/20\n  Batch 0: Loss = 5.7907\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: ''''ङ'ङ''ङङङ'ङ''ङङङङङङ''ङ'ङङ''...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 3.3982\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction:                               ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 1 completed. Average loss: 3.5007\n==================================================\n\nEpoch 2/20\n  Batch 0: Loss = 3.3832\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: म                             ...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 3.0471\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: पर   क  का    प   र    कर र   ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 2 completed. Average loss: 3.1609\n==================================================\n\nEpoch 3/20\n  Batch 0: Loss = 2.8761\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: स्क्र  क  क् र् े ी क ाकेरा का...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.7835\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: सरर् कर्का् ारकेर्रे राकसार   ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 3 completed. Average loss: 2.8092\n==================================================\n\nEpoch 4/20\n  Batch 0: Loss = 2.7558\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: इिर्रा के नााया     ककिकेस् का...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.6953\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: पार् सर कर  ा के  र    कन र   ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 4 completed. Average loss: 2.7126\n==================================================\n\nEpoch 5/20\n  Batch 0: Loss = 2.6847\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: अास्र् कै काेरेाा ् कपाक्त् से...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.6190\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: विर  कर कीा  रकान रा य कप्रा  ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 5 completed. Average loss: 2.6336\n==================================================\n\nEpoch 6/20\n  Batch 0: Loss = 2.6038\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: इिक्रि पै काार ्ि   कसाकि ् के...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.5659\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: इिर् कर काि   किर्रााय कन रां ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 6 completed. Average loss: 2.5808\n==================================================\n\nEpoch 7/20\n  Batch 0: Loss = 2.5731\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: मेस्यि कम पाेये     कपाकी े के...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.5264\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: विर्रकर की्   किर्र  क कप्रां ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 7 completed. Average loss: 2.5325\n==================================================\n\nEpoch 8/20\n  Batch 0: Loss = 2.5230\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: इिर्र् कम को्य  ्   कपाके ् के...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.4675\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: विरि कर की् ारसिर्र ि  कप्य ं ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 8 completed. Average loss: 2.4828\n==================================================\n\nEpoch 9/20\n  Batch 0: Loss = 2.4794\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: इेस्र  कम कोेते ा   कराके ् पि...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.4200\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: कारा कर की्   किर्त  क कर्यां ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 9 completed. Average loss: 2.4301\n==================================================\n\nEpoch 10/20\n  Batch 0: Loss = 2.4308\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: येक्त  कम काेय  ्   कत्के ् कि...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.3646\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: इारा कर की् ा कहम्त िक कप्यां ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 10 completed. Average loss: 2.3765\n==================================================\n\nEpoch 11/20\n  Batch 0: Loss = 2.3688\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: इिन्या कम कोेह ्ि   कताके ् कि...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.3067\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: यिरा कर सीा ारकाल्ति य सप्वां ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 11 completed. Average loss: 2.3261\n==================================================\n\nEpoch 12/20\n  Batch 0: Loss = 2.3299\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: इिन्या कम जिेन ्ि   करिके ् के...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.2542\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: किर्रकर कीा  नकिर्ति   कप्थां ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 12 completed. Average loss: 2.2773\n==================================================\n\nEpoch 13/20\n  Batch 0: Loss = 2.2716\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: सिक्रा कम जि्ह ्ा   कराके   कि...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.2179\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: केरा कर कीा  नकिल ति क कप्गां ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 13 completed. Average loss: 2.2319\n==================================================\n\nEpoch 14/20\n  Batch 0: Loss = 2.2447\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: इेक्र  कम को्ह  ा   कत्के   पि...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.1861\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: पेर  कर कीा   किल्थि क कप्थां ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 14 completed. Average loss: 2.1896\n==================================================\n\nEpoch 15/20\n  Batch 0: Loss = 2.2058\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: सिश्ध  कै को्य ्ा   ककाके   कि...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.1500\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: किर रकर कीा   किल तु क कन्ण ं ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 15 completed. Average loss: 2.1519\n==================================================\n\nEpoch 16/20\n  Batch 0: Loss = 2.1762\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: इिर्ध् कम कोेत ाा   कर के ी सि...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.1033\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: कुर  कर कीा   किल त  क कप्थिं ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 16 completed. Average loss: 2.1158\n==================================================\n\nEpoch 17/20\n  Batch 0: Loss = 2.1461\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: किय्च् की को्ह िार  कप के ी कि...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.0820\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: पुर रकर की्   किल्त  क सप्थां ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 17 completed. Average loss: 2.0835\n==================================================\n\nEpoch 18/20\n  Batch 0: Loss = 2.1211\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: अिर्र् की जा्यााारी कयीके ् कि...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.0542\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: कुरी कर कीा   किल त  क कन्थां ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 18 completed. Average loss: 2.0537\n==================================================\n\nEpoch 19/20\n  Batch 0: Loss = 2.0781\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: यिर्चा की को्ह ााली कवाके ी के...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 2.0076\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: किन  कर कीा  नकिल त  क कप्थां ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 19 completed. Average loss: 2.0245\n==================================================\n\nEpoch 20/20\n  Batch 0: Loss = 2.0637\n  English: indeed the companions of parad...\n  Hindi: निश्चय ही जन्नतवाले आज किसी न ...\n  Prediction: इेर्चे की को्ह ााल  कप के ी कि...\n  ----------------------------------------\nError in batch 8: Invalid token index: 100\n  Batch 100: Loss = 1.9813\n  English: the uncle and nephew were guru...\n  Hindi: चाचा और भतीजा वास्तविक अर्थों ...\n  Prediction: कारीरकर सी्   किल्तु क कन्थ ं ...\n  ----------------------------------------\nError in batch 145: Invalid token index: 100\nEpoch 20 completed. Average loss: 1.9982\n==================================================\n\nTraining completed!\nYou can now test the model or continue training with more epochs and larger datasets.\n\n============================================================\nTESTING THE MODEL\n============================================================\nInput: hello how are you\nTranslation: क्या है कि साथ करें<END>\n============================================================\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"## saving model","metadata":{}},{"cell_type":"code","source":"# Save the trained model\nimport pickle\nimport json\nfrom datetime import datetime\n\ndef save_model(model, model_path=\"/kaggle/working/english_hindi_transformer_10000val_20epoch.pth\", config_path=\"model_config.json\"):\n    \"\"\"\n    Save the trained transformer model and its configuration\n    \"\"\"\n    print(\"Saving model...\")\n    \n    # Save model state dict\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'model_config': {\n            'd_model': d_model,\n            'ffn_hidden': ffn_hidden,\n            'num_heads': num_heads,\n            'drop_prob': drop_prob,\n            'num_layers': num_layers,\n            'max_sequence_length': max_sequence_length,\n            'hin_vocab_size': hin_vocab_size,\n        },\n        'vocabularies': {\n            'hindi_vocabulary': hindi_vocabulary,\n            'english_vocabulary': english_vocabulary,\n            'hindi_to_index': hindi_to_index,\n            'english_to_index': english_to_index,\n            'index_to_hindi': index_to_hindi,\n            'index_to_english': index_to_english,\n        },\n        'tokens': {\n            'START_TOKEN': START_TOKEN,\n            'END_TOKEN': END_TOKEN,\n            'PADDING_TOKEN': PADDING_TOKEN,\n            'UNK_TOKEN': UNK_TOKEN,\n        },\n        'training_info': {\n            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            'num_epochs_trained': num_epochs,\n            'final_loss': avg_loss if 'avg_loss' in locals() else None,\n        }\n    }, model_path)\n    \n    print(f\"Model saved to: {model_path}\")\n    print(\"Saved components:\")\n    print(\"- Model state dict\")\n    print(\"- Model configuration\")\n    print(\"- Vocabularies and mappings\")\n    print(\"- Special tokens\")\n    print(\"- Training information\")\n\n# Save the model\nsave_model(transformer)\nprint(\"Model saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:36:59.448708Z","iopub.execute_input":"2025-08-26T20:36:59.449407Z","iopub.status.idle":"2025-08-26T20:36:59.573047Z","shell.execute_reply.started":"2025-08-26T20:36:59.449383Z","shell.execute_reply":"2025-08-26T20:36:59.572313Z"}},"outputs":[{"name":"stdout","text":"Saving model...\nModel saved to: /kaggle/working/english_hindi_transformer_10000val_20epoch.pth\nSaved components:\n- Model state dict\n- Model configuration\n- Vocabularies and mappings\n- Special tokens\n- Training information\nModel saved successfully!\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# Load and use the saved model for translation\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\n\ndef load_model(model_path=\"/kaggle/working/english_hindi_transformer_fullset_100epoch.pth\"):\n    \"\"\"\n    Load the saved transformer model and its configuration\n    \"\"\"\n    print(\"Loading model...\")\n    \n    # Load the saved data\n    checkpoint = torch.load(model_path, map_location='cpu')\n    \n    # Extract configuration\n    config = checkpoint['model_config']\n    vocabularies = checkpoint['vocabularies']\n    tokens = checkpoint['tokens']\n    \n    # Recreate the model architecture (copy the classes from above)\n    # You need to make sure all the transformer classes are defined before calling this\n    \n    # Create model instance\n    model = Transformer(\n        d_model=config['d_model'],\n        ffn_hidden=config['ffn_hidden'],\n        num_heads=config['num_heads'],\n        drop_prob=config['drop_prob'],\n        num_layers=config['num_layers'],\n        max_sequence_length=config['max_sequence_length'],\n        hin_vocab_size=config['hin_vocab_size'],\n        english_to_index=vocabularies['english_to_index'],\n        hindi_to_index=vocabularies['hindi_to_index'],\n        START_TOKEN=tokens['START_TOKEN'],\n        END_TOKEN=tokens['END_TOKEN'],\n        PADDING_TOKEN=tokens['PADDING_TOKEN']\n    )\n    \n    # Load the state dict\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Set to evaluation mode\n    model.eval()\n    \n    print(\"Model loaded successfully!\")\n    print(f\"Training info: {checkpoint['training_info']}\")\n    \n    return model, vocabularies, tokens, config\n\ndef translate_sentence(model, english_sentence, vocabularies, tokens, config, device='cpu'):\n    \"\"\"\n    Translate an English sentence to Hindi using the loaded model\n    \"\"\"\n    model.to(device)\n    model.eval()\n    \n    # Get vocabularies and tokens\n    hindi_to_index = vocabularies['hindi_to_index']\n    english_to_index = vocabularies['english_to_index']\n    index_to_hindi = vocabularies['index_to_hindi']\n    START_TOKEN = tokens['START_TOKEN']\n    END_TOKEN = tokens['END_TOKEN']\n    PADDING_TOKEN = tokens['PADDING_TOKEN']\n    max_sequence_length = config['max_sequence_length']\n    \n    # Create masks function (simplified version)\n    def create_simple_masks(eng_sentence, hin_sentence, max_len):\n        NEG_INFTY = -1e9\n        num_sentences = 1\n        \n        look_ahead_mask = torch.full([max_len, max_len], True)\n        look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n        \n        encoder_padding_mask = torch.full([num_sentences, max_len, max_len], False)\n        decoder_padding_mask_self_attention = torch.full([num_sentences, max_len, max_len], False)\n        decoder_padding_mask_cross_attention = torch.full([num_sentences, max_len, max_len], False)\n        \n        eng_sentence_length = len(eng_sentence)\n        hin_sentence_length = len(hin_sentence)\n        \n        eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_len)\n        hin_chars_to_padding_mask = np.arange(hin_sentence_length + 1, max_len)\n        \n        encoder_padding_mask[0, :, eng_chars_to_padding_mask] = True\n        encoder_padding_mask[0, eng_chars_to_padding_mask, :] = True\n        decoder_padding_mask_self_attention[0, :, hin_chars_to_padding_mask] = True\n        decoder_padding_mask_self_attention[0, hin_chars_to_padding_mask, :] = True\n        decoder_padding_mask_cross_attention[0, :, eng_chars_to_padding_mask] = True\n        decoder_padding_mask_cross_attention[0, hin_chars_to_padding_mask, :] = True\n        \n        encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n        decoder_self_attention_mask = torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n        decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n        \n        return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n    \n    print(f\"Translating: '{english_sentence}'\")\n    \n    with torch.no_grad():\n        # Generate translation\n        hin_sentence = \"\"\n        eng_sentence_tuple = (english_sentence.lower(),)\n        \n        for word_counter in range(min(100, max_sequence_length)):\n            hin_sentence_tuple = (hin_sentence,)\n            \n            # Create masks\n            encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_simple_masks(\n                english_sentence, hin_sentence, max_sequence_length\n            )\n            \n            # Get predictions\n            predictions = model(\n                eng_sentence_tuple,\n                hin_sentence_tuple,\n                encoder_self_attention_mask.to(device),\n                decoder_self_attention_mask.to(device),\n                decoder_cross_attention_mask.to(device),\n                enc_start_token=False,\n                enc_end_token=False,\n                dec_start_token=True,\n                dec_end_token=False\n            )\n            \n            # Get next token\n            next_token_prob_distribution = predictions[0][word_counter]\n            next_token_index = torch.argmax(next_token_prob_distribution).item()\n            next_token = index_to_hindi[next_token_index]\n            \n            # Add token to sentence\n            hin_sentence += next_token\n            \n            # Check for end token\n            if next_token == END_TOKEN:\n                break\n    \n    return hin_sentence.replace(END_TOKEN, \"\").strip()\n\n# Example usage - Load model and translate\ntry:\n    # Load the model\n    loaded_model, loaded_vocabularies, loaded_tokens, loaded_config = load_model()\n    \n    # Test translations\n    test_sentences = [\n        \"hello how are you\",\n        \"what is your name\",\n        \"good morning\",\n        \"thank you very much\",\n        \"i love you\"\n    ]\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"TESTING LOADED MODEL\")\n    print(\"=\"*60)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    for sentence in test_sentences:\n        translation = translate_sentence(loaded_model, sentence, loaded_vocabularies, loaded_tokens, loaded_config, device)\n        print(f\"English: {sentence}\")\n        print(f\"Hindi: {translation}\")\n        print(\"-\" * 40)\n        \nexcept FileNotFoundError:\n    print(\"Model file not found. Please train and save the model first.\")\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n    print(\"Make sure to run the training cell first to create the model.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T19:46:03.753823Z","iopub.status.idle":"2025-08-26T19:46:03.754049Z","shell.execute_reply.started":"2025-08-26T19:46:03.753952Z","shell.execute_reply":"2025-08-26T19:46:03.753962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}